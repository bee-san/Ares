//! OpenAI-compatible HTTP client for AI features.
//!
//! This module provides a client that can communicate with any OpenAI-compatible
//! API endpoint (OpenAI, Ollama, LM Studio, Azure OpenAI, etc.) using the
//! standard chat completions API.
//!
//! Features:
//! - Configurable connect and request timeouts
//! - Automatic retry with exponential backoff for transient errors (5xx)
//! - Dedicated handling of 429 (rate limited) responses

use std::thread;
use std::time::Duration;

use serde::{Deserialize, Serialize};

use crate::config::Config;

use super::error::AiError;

/// Maximum number of retries for transient (5xx) errors.
const MAX_RETRIES: u32 = 2;
/// Base delay for exponential backoff between retries (in milliseconds).
const RETRY_BASE_DELAY_MS: u64 = 1000;
/// Connect timeout for HTTP requests (in seconds).
const CONNECT_TIMEOUT_SECS: u64 = 10;
/// Total request timeout for HTTP requests (in seconds).
const REQUEST_TIMEOUT_SECS: u64 = 60;

/// A message in a chat completion request.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChatMessage {
    /// The role of the message author ("system", "user", or "assistant").
    pub role: String,
    /// The content of the message.
    pub content: String,
}

impl ChatMessage {
    /// Creates a new system message.
    pub fn system(content: &str) -> Self {
        Self {
            role: "system".to_string(),
            content: content.to_string(),
        }
    }

    /// Creates a new user message.
    pub fn user(content: &str) -> Self {
        Self {
            role: "user".to_string(),
            content: content.to_string(),
        }
    }

    /// Creates a new assistant message.
    pub fn assistant(content: &str) -> Self {
        Self {
            role: "assistant".to_string(),
            content: content.to_string(),
        }
    }
}

/// Request body for the chat completions endpoint.
#[derive(Debug, Serialize)]
struct ChatCompletionRequest {
    /// The model to use for completion.
    model: String,
    /// The messages to send to the model.
    messages: Vec<ChatMessage>,
    /// Sampling temperature (0.0-2.0). Lower = more deterministic.
    #[serde(skip_serializing_if = "Option::is_none")]
    temperature: Option<f32>,
    /// Maximum tokens to generate in the response.
    #[serde(skip_serializing_if = "Option::is_none")]
    max_tokens: Option<u32>,
}

/// A single choice in the chat completion response.
#[derive(Debug, Deserialize)]
struct ChatCompletionChoice {
    /// The message generated by the model.
    message: ChatMessage,
}

/// Response body from the chat completions endpoint.
#[derive(Debug, Deserialize)]
struct ChatCompletionResponse {
    /// List of completion choices (usually one).
    choices: Vec<ChatCompletionChoice>,
}

/// Error response body from the API.
#[derive(Debug, Deserialize)]
struct ApiErrorResponse {
    /// The error details.
    error: ApiErrorDetail,
}

/// Detail of an API error.
#[derive(Debug, Deserialize)]
struct ApiErrorDetail {
    /// The error message.
    message: String,
}

/// Client for communicating with an OpenAI-compatible API endpoint.
#[derive(Debug, Clone)]
pub struct AiClient {
    /// The base URL for the API (e.g., "https://api.openai.com/v1").
    api_url: String,
    /// The API key for authentication.
    api_key: String,
    /// The model name to use (e.g., "gpt-4o-mini").
    model: String,
}

impl AiClient {
    /// Creates a new `AiClient` from the given configuration.
    ///
    /// Returns `None` if AI is not fully configured (missing URL, key, or model).
    pub fn from_config(config: &Config) -> Option<Self> {
        let api_url = config.ai_api_url.as_ref()?;
        let api_key = config.ai_api_key.as_ref()?;
        let model = config.ai_model.as_ref()?;

        if api_url.is_empty() || api_key.is_empty() || model.is_empty() {
            return None;
        }

        Some(Self {
            api_url: api_url.clone(),
            api_key: api_key.clone(),
            model: model.clone(),
        })
    }

    /// Sends a chat completion request and returns the assistant's response text.
    ///
    /// Includes automatic retry with exponential backoff for transient (5xx) errors.
    /// Does not retry client errors (4xx) except 429 which is surfaced as `RateLimited`.
    ///
    /// # Arguments
    ///
    /// * `messages` - The conversation messages to send
    /// * `temperature` - Optional sampling temperature (defaults to model's default)
    /// * `max_tokens` - Optional maximum tokens for the response
    ///
    /// # Errors
    ///
    /// Returns `AiError::Timeout` if the request times out, `AiError::RateLimited`
    /// for 429 responses, `AiError::HttpError` for network failures,
    /// `AiError::ApiError` for other non-success status codes, or
    /// `AiError::ParseError` if the response cannot be parsed.
    pub fn chat_completion(
        &self,
        messages: Vec<ChatMessage>,
        temperature: Option<f32>,
        max_tokens: Option<u32>,
    ) -> Result<String, AiError> {
        let url = format!("{}/chat/completions", self.api_url.trim_end_matches('/'));

        let request_body = ChatCompletionRequest {
            model: self.model.clone(),
            messages,
            temperature,
            max_tokens,
        };

        let client = reqwest::blocking::Client::builder()
            .connect_timeout(Duration::from_secs(CONNECT_TIMEOUT_SECS))
            .timeout(Duration::from_secs(REQUEST_TIMEOUT_SECS))
            .build()
            .map_err(|e| AiError::HttpError(e.to_string()))?;

        let mut last_error: Option<AiError> = None;

        for attempt in 0..=MAX_RETRIES {
            if attempt > 0 {
                // Exponential backoff: 1s, 2s
                let delay_ms = RETRY_BASE_DELAY_MS * (1 << (attempt - 1));
                thread::sleep(Duration::from_millis(delay_ms));
            }

            let response = match client
                .post(&url)
                .header("Content-Type", "application/json")
                .header("Authorization", format!("Bearer {}", self.api_key))
                .json(&request_body)
                .send()
            {
                Ok(resp) => resp,
                Err(e) => {
                    let ai_err: AiError = e.into();
                    // Don't retry timeouts  they'll just timeout again
                    if matches!(ai_err, AiError::Timeout) {
                        return Err(ai_err);
                    }
                    last_error = Some(ai_err);
                    continue;
                }
            };

            let status = response.status().as_u16();

            // Handle 429 (rate limited)  don't retry, surface immediately
            if status == 429 {
                let retry_after = response
                    .headers()
                    .get("retry-after")
                    .and_then(|v| v.to_str().ok())
                    .and_then(|v| v.parse::<u64>().ok());
                return Err(AiError::RateLimited { retry_after });
            }

            let body = match response.text() {
                Ok(b) => b,
                Err(e) => {
                    last_error = Some(e.into());
                    continue;
                }
            };

            // Retry on 5xx (server) errors
            if status >= 500 {
                let message = match serde_json::from_str::<ApiErrorResponse>(&body) {
                    Ok(err_resp) => err_resp.error.message,
                    Err(_) => body,
                };
                last_error = Some(AiError::ApiError { status, message });
                continue;
            }

            // Don't retry 4xx (client) errors
            if status != 200 {
                let message = match serde_json::from_str::<ApiErrorResponse>(&body) {
                    Ok(err_resp) => err_resp.error.message,
                    Err(_) => body,
                };
                return Err(AiError::ApiError { status, message });
            }

            // Success  parse response
            let parsed: ChatCompletionResponse = serde_json::from_str(&body)?;
            let content = parsed
                .choices
                .into_iter()
                .next()
                .map(|c| c.message.content)
                .unwrap_or_default();

            return Ok(content);
        }

        // All retries exhausted
        Err(last_error.unwrap_or_else(|| AiError::HttpError("All retries exhausted".to_string())))
    }

    /// Returns the configured model name.
    pub fn model(&self) -> &str {
        &self.model
    }

    /// Returns the configured API URL.
    pub fn api_url(&self) -> &str {
        &self.api_url
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_chat_message_system() {
        let msg = ChatMessage::system("You are a helpful assistant.");
        assert_eq!(msg.role, "system");
        assert_eq!(msg.content, "You are a helpful assistant.");
    }

    #[test]
    fn test_chat_message_user() {
        let msg = ChatMessage::user("Hello!");
        assert_eq!(msg.role, "user");
        assert_eq!(msg.content, "Hello!");
    }

    #[test]
    fn test_chat_message_assistant() {
        let msg = ChatMessage::assistant("I can help with that.");
        assert_eq!(msg.role, "assistant");
        assert_eq!(msg.content, "I can help with that.");
    }

    #[test]
    fn test_ai_client_from_config_complete() {
        let mut config = Config::default();
        config.ai_enabled = true;
        config.ai_api_url = Some("https://api.openai.com/v1".to_string());
        config.ai_api_key = Some("sk-test-key".to_string());
        config.ai_model = Some("gpt-4o-mini".to_string());

        let client = AiClient::from_config(&config);
        assert!(client.is_some());
        let client = client.unwrap();
        assert_eq!(client.model(), "gpt-4o-mini");
        assert_eq!(client.api_url(), "https://api.openai.com/v1");
    }

    #[test]
    fn test_ai_client_from_config_missing_url() {
        let mut config = Config::default();
        config.ai_enabled = true;
        config.ai_api_key = Some("sk-test-key".to_string());
        config.ai_model = Some("gpt-4o-mini".to_string());
        // ai_api_url is None

        let client = AiClient::from_config(&config);
        assert!(client.is_none());
    }

    #[test]
    fn test_ai_client_from_config_empty_key() {
        let mut config = Config::default();
        config.ai_enabled = true;
        config.ai_api_url = Some("https://api.openai.com/v1".to_string());
        config.ai_api_key = Some(String::new()); // empty
        config.ai_model = Some("gpt-4o-mini".to_string());

        let client = AiClient::from_config(&config);
        assert!(client.is_none());
    }

    #[test]
    fn test_chat_completion_request_serialization() {
        let request = ChatCompletionRequest {
            model: "gpt-4o-mini".to_string(),
            messages: vec![
                ChatMessage::system("You are helpful."),
                ChatMessage::user("Hi"),
            ],
            temperature: Some(0.3),
            max_tokens: Some(1024),
        };

        let json = serde_json::to_string(&request).unwrap();
        assert!(json.contains("gpt-4o-mini"));
        assert!(json.contains("system"));
        assert!(json.contains("user"));
        assert!(json.contains("temperature"));
    }

    #[test]
    fn test_chat_completion_request_serialization_no_max_tokens() {
        let request = ChatCompletionRequest {
            model: "gpt-4o-mini".to_string(),
            messages: vec![ChatMessage::user("Hi")],
            temperature: None,
            max_tokens: None,
        };

        let json = serde_json::to_string(&request).unwrap();
        assert!(!json.contains("max_tokens"));
        assert!(!json.contains("temperature"));
    }

    #[test]
    fn test_chat_completion_response_deserialization() {
        let json = r#"{
            "id": "chatcmpl-test",
            "object": "chat.completion",
            "created": 1234567890,
            "model": "gpt-4o-mini",
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": "Hello! How can I help?"
                },
                "finish_reason": "stop"
            }],
            "usage": {"prompt_tokens": 10, "completion_tokens": 8, "total_tokens": 18}
        }"#;

        let response: ChatCompletionResponse = serde_json::from_str(json).unwrap();
        assert_eq!(response.choices.len(), 1);
        assert_eq!(
            response.choices[0].message.content,
            "Hello! How can I help?"
        );
    }
}
