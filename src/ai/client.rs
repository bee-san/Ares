//! OpenAI-compatible HTTP client for AI features.
//!
//! This module provides a client that can communicate with any OpenAI-compatible
//! API endpoint (OpenAI, Ollama, LM Studio, Azure OpenAI, etc.) using the
//! standard chat completions API.

use serde::{Deserialize, Serialize};

use crate::config::Config;

use super::error::AiError;

/// A message in a chat completion request.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChatMessage {
    /// The role of the message author ("system", "user", or "assistant").
    pub role: String,
    /// The content of the message.
    pub content: String,
}

impl ChatMessage {
    /// Creates a new system message.
    pub fn system(content: &str) -> Self {
        Self {
            role: "system".to_string(),
            content: content.to_string(),
        }
    }

    /// Creates a new user message.
    pub fn user(content: &str) -> Self {
        Self {
            role: "user".to_string(),
            content: content.to_string(),
        }
    }
}

/// Request body for the chat completions endpoint.
#[derive(Debug, Serialize)]
struct ChatCompletionRequest {
    /// The model to use for completion.
    model: String,
    /// The messages to send to the model.
    messages: Vec<ChatMessage>,
    /// Sampling temperature (0.0-2.0). Lower = more deterministic.
    #[serde(skip_serializing_if = "Option::is_none")]
    temperature: Option<f32>,
    /// Maximum tokens to generate in the response.
    #[serde(skip_serializing_if = "Option::is_none")]
    max_tokens: Option<u32>,
}

/// A single choice in the chat completion response.
#[derive(Debug, Deserialize)]
struct ChatCompletionChoice {
    /// The message generated by the model.
    message: ChatMessage,
}

/// Response body from the chat completions endpoint.
#[derive(Debug, Deserialize)]
struct ChatCompletionResponse {
    /// List of completion choices (usually one).
    choices: Vec<ChatCompletionChoice>,
}

/// Error response body from the API.
#[derive(Debug, Deserialize)]
struct ApiErrorResponse {
    /// The error details.
    error: ApiErrorDetail,
}

/// Detail of an API error.
#[derive(Debug, Deserialize)]
struct ApiErrorDetail {
    /// The error message.
    message: String,
}

/// Client for communicating with an OpenAI-compatible API endpoint.
#[derive(Debug, Clone)]
pub struct AiClient {
    /// The base URL for the API (e.g., "https://api.openai.com/v1").
    api_url: String,
    /// The API key for authentication.
    api_key: String,
    /// The model name to use (e.g., "gpt-4o-mini").
    model: String,
}

impl AiClient {
    /// Creates a new `AiClient` from the given configuration.
    ///
    /// Returns `None` if AI is not fully configured (missing URL, key, or model).
    pub fn from_config(config: &Config) -> Option<Self> {
        let api_url = config.ai_api_url.as_ref()?;
        let api_key = config.ai_api_key.as_ref()?;
        let model = config.ai_model.as_ref()?;

        if api_url.is_empty() || api_key.is_empty() || model.is_empty() {
            return None;
        }

        Some(Self {
            api_url: api_url.clone(),
            api_key: api_key.clone(),
            model: model.clone(),
        })
    }

    /// Sends a chat completion request and returns the assistant's response text.
    ///
    /// # Arguments
    ///
    /// * `messages` - The conversation messages to send
    /// * `temperature` - Optional sampling temperature (defaults to model's default)
    ///
    /// # Errors
    ///
    /// Returns `AiError::HttpError` if the request fails, `AiError::ApiError` if the
    /// API returns a non-success status code, or `AiError::ParseError` if the response
    /// cannot be parsed.
    pub fn chat_completion(
        &self,
        messages: Vec<ChatMessage>,
        temperature: Option<f32>,
    ) -> Result<String, AiError> {
        let url = format!("{}/chat/completions", self.api_url.trim_end_matches('/'));

        let request_body = ChatCompletionRequest {
            model: self.model.clone(),
            messages,
            temperature,
            max_tokens: Some(2048),
        };

        let client = reqwest::blocking::Client::new();
        let response = client
            .post(&url)
            .header("Content-Type", "application/json")
            .header("Authorization", format!("Bearer {}", self.api_key))
            .json(&request_body)
            .send()?;

        let status = response.status().as_u16();
        let body = response.text()?;

        if status != 200 {
            // Try to parse error response for a helpful message
            let message = match serde_json::from_str::<ApiErrorResponse>(&body) {
                Ok(err_resp) => err_resp.error.message,
                Err(_) => body,
            };
            return Err(AiError::ApiError { status, message });
        }

        let parsed: ChatCompletionResponse = serde_json::from_str(&body)?;
        let content = parsed
            .choices
            .into_iter()
            .next()
            .map(|c| c.message.content)
            .unwrap_or_default();

        Ok(content)
    }

    /// Returns the configured model name.
    pub fn model(&self) -> &str {
        &self.model
    }

    /// Returns the configured API URL.
    pub fn api_url(&self) -> &str {
        &self.api_url
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_chat_message_system() {
        let msg = ChatMessage::system("You are a helpful assistant.");
        assert_eq!(msg.role, "system");
        assert_eq!(msg.content, "You are a helpful assistant.");
    }

    #[test]
    fn test_chat_message_user() {
        let msg = ChatMessage::user("Hello!");
        assert_eq!(msg.role, "user");
        assert_eq!(msg.content, "Hello!");
    }

    #[test]
    fn test_ai_client_from_config_complete() {
        let mut config = Config::default();
        config.ai_enabled = true;
        config.ai_api_url = Some("https://api.openai.com/v1".to_string());
        config.ai_api_key = Some("sk-test-key".to_string());
        config.ai_model = Some("gpt-4o-mini".to_string());

        let client = AiClient::from_config(&config);
        assert!(client.is_some());
        let client = client.unwrap();
        assert_eq!(client.model(), "gpt-4o-mini");
        assert_eq!(client.api_url(), "https://api.openai.com/v1");
    }

    #[test]
    fn test_ai_client_from_config_missing_url() {
        let mut config = Config::default();
        config.ai_enabled = true;
        config.ai_api_key = Some("sk-test-key".to_string());
        config.ai_model = Some("gpt-4o-mini".to_string());
        // ai_api_url is None

        let client = AiClient::from_config(&config);
        assert!(client.is_none());
    }

    #[test]
    fn test_ai_client_from_config_empty_key() {
        let mut config = Config::default();
        config.ai_enabled = true;
        config.ai_api_url = Some("https://api.openai.com/v1".to_string());
        config.ai_api_key = Some(String::new()); // empty
        config.ai_model = Some("gpt-4o-mini".to_string());

        let client = AiClient::from_config(&config);
        assert!(client.is_none());
    }

    #[test]
    fn test_chat_completion_request_serialization() {
        let request = ChatCompletionRequest {
            model: "gpt-4o-mini".to_string(),
            messages: vec![
                ChatMessage::system("You are helpful."),
                ChatMessage::user("Hi"),
            ],
            temperature: Some(0.3),
            max_tokens: Some(1024),
        };

        let json = serde_json::to_string(&request).unwrap();
        assert!(json.contains("gpt-4o-mini"));
        assert!(json.contains("system"));
        assert!(json.contains("user"));
        assert!(json.contains("temperature"));
    }

    #[test]
    fn test_chat_completion_response_deserialization() {
        let json = r#"{
            "id": "chatcmpl-test",
            "object": "chat.completion",
            "created": 1234567890,
            "model": "gpt-4o-mini",
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": "Hello! How can I help?"
                },
                "finish_reason": "stop"
            }],
            "usage": {"prompt_tokens": 10, "completion_tokens": 8, "total_tokens": 18}
        }"#;

        let response: ChatCompletionResponse = serde_json::from_str(json).unwrap();
        assert_eq!(response.choices.len(), 1);
        assert_eq!(
            response.choices[0].message.content,
            "Hello! How can I help?"
        );
    }
}
